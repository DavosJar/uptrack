package scheduler

import (
	"fmt"
	"log"
	"uptrackai/internal/monitoring/domain"
	notificationdomain "uptrackai/internal/notifications/domain"
)

// NotificationChecker interface to check if a user has active notification channels
type NotificationChecker interface {
	HasActiveChannel(userId string) bool
}

type Orchestrator struct {
	config              OrchestratorConfig
	healthChecker       *HealthChecker
	metricsCalc         *MetricsCalculator
	resultAnalyzer      *ResultAnalyzer
	stateUpdater        *StateUpdater
	dispatcher          *NotificationDispatcher
	statsRepo           domain.TargetStatisticsRepository
	notificationChecker NotificationChecker
	severityMapper      *notificationdomain.SeverityMapper

	workerPool *WorkerPool
}

type OrchestratorConfig struct {
	WorkerCount int
	BufferSize  int
}

func NewOrchestrator(
	config OrchestratorConfig,
	targetRepo domain.MonitoringTargetRepository,
	metricsRepo domain.MetricsRepository,
	checkRepo domain.CheckResultRepository,
	statsRepo domain.TargetStatisticsRepository,
	dispatcher *NotificationDispatcher,
	notificationChecker NotificationChecker,
) *Orchestrator {
	orch := &Orchestrator{
		config:              config,
		healthChecker:       NewHealthChecker(),
		metricsCalc:         NewMetricsCalculator(),
		resultAnalyzer:      NewResultAnalyzer(),
		stateUpdater:        NewStateUpdater(targetRepo, metricsRepo, checkRepo),
		dispatcher:          dispatcher,
		statsRepo:           statsRepo,
		notificationChecker: notificationChecker,
		severityMapper:      notificationdomain.NewSeverityMapper(),
	}

	// Create worker pool with processing function
	workerPoolConfig := WorkerPoolConfig{
		WorkerCount: config.WorkerCount,
		BufferSize:  config.BufferSize,
	}
	orch.workerPool = NewWorkerPool(workerPoolConfig, orch.processTarget)

	return orch
}

// Start inicia el pool de workers
func (o *Orchestrator) Start() {
	o.workerPool.Start()
}

// Stop detiene el orchestrator y espera a que terminen los workers
func (o *Orchestrator) Stop() {
	log.Println("Deteniendo Scheduler Orchestrator...")
	o.workerPool.Stop()
	log.Println("Scheduler Orchestrator detenido")
}

// Schedule agrega una lista de targets para ser procesados
func (o *Orchestrator) Schedule(targets []*domain.MonitoringTarget) {
	o.workerPool.SubmitBatch(targets)
}

func (o *Orchestrator) processTarget(target *domain.MonitoringTarget) {
	// 1. Health Check
	session := o.healthChecker.Check(target)

	// 2. Calcular M칠tricas
	metrics := o.metricsCalc.Calculate(session)

	// 3. Obtener Hist칩rico (para an치lisis)
	historical, err := o.statsRepo.Get(target.ID())
	if err != nil {
		// Si falla, usamos uno vac칤o para no detener el proceso
		historical = domain.NewTargetStatistics(target.ID())
	}

	// 4. Analizar Resultados
	newStatus := o.resultAnalyzer.Analyze(session, metrics, historical)

	// Capturar estado previo para detectar cambios (Eventos)
	previousStatus := target.CurrentStatus()

	// 5. Actualizar Estado (DB & Memoria)
	o.stateUpdater.Update(target, newStatus, metrics)

	// 6. Actualizar Estad칤sticas Hist칩ricas (Async o Sync?)
	// Lo hacemos aqu칤 sync por simplicidad, pero podr칤a ser otro job
	checkInterval := target.Configuration().CheckIntervalSeconds()
	if checkInterval <= 0 {
		checkInterval = 300
	}

	const WINDOW_DAYS = 7
	const SECONDS_IN_DAY = 86400
	maxChecks := (WINDOW_DAYS * SECONDS_IN_DAY) / checkInterval

	// 1. Calcular nuevo promedio (Matem치tica pura)
	// Solo actualizamos el promedio si la sesi칩n fue estable y r치pida (<= 4 pings)
	// Si tom칩 m치s pings (ej. 7), el sistema est치 inestable y no debe ensuciar la l칤nea base.
	currentAvg := historical.AvgResponseTimeMs()
	newAvg := currentAvg

	if metrics.TotalChecks <= 4 {
		newAvg = domain.CalculateNewAverage(
			currentAvg,
			historical.TotalChecksCount(),
			metrics.AvgResponseTimeMs,
			1, // 1 Check Session
		)
	}

	// 2. Actualizar estado (Mutaci칩n)
	historical.UpdateState(newAvg, maxChecks)
	_ = o.statsRepo.Save(historical)

	// 7. Notificar si es necesario
	if o.notificationChecker != nil && o.notificationChecker.HasActiveChannel(target.UserId().String()) {
		newSeverity := o.severityMapper.Map(string(newStatus))
		prevSeverity := o.severityMapper.Map(string(previousStatus))

		message := fmt.Sprintf("Target %s is now %s", target.Name(), newStatus)

		event := notificationdomain.NewAlertEvent(
			target.UserId().String(),
			"Status Change: "+target.Name(),
			message,
			newSeverity,
			prevSeverity,
			"Target: "+target.Name(),
			notificationdomain.AlertTypeMonitoring,
			map[string]string{
				"url":           target.Url(),
				"response_time": fmt.Sprintf("%dms", metrics.AvgResponseTimeMs),
			},
		)

		if event.ShouldNotify() {
			if o.dispatcher != nil {
				o.dispatcher.Dispatch(*event)
				log.Printf("游닉 ALERT DISPATCHED | Target: %s | Severity: %s", target.Name(), newSeverity)
			}
		}
	}

	// 8. Log de Transici칩n de Estado (Solo si hubo cambio relevante)
	// Si el estado cambi칩, imprimimos la transici칩n clara: PREV -> NEW
	// Esto valida que la l칩gica de detecci칩n funciona, independientemente de si se notifica o no.
	if previousStatus != newStatus {
		log.Printf("游댃 STATE_CHANGE | Target: %s (%s) | %s 俱뫮잺  %s | Time: %dms",
			target.Name(), target.Url(), previousStatus, newStatus, metrics.AvgResponseTimeMs)
	}
}

// RunBatch ejecuta un lote de targets de manera as칤ncrona
func (o *Orchestrator) RunBatch(targets []*domain.MonitoringTarget) {
	// Enviar trabajos de manera as칤ncrona
	o.Schedule(targets)

	// NO esperamos - los workers procesan en background
	// El pr칩ximo ciclo del scheduler vendr치 en el intervalo configurado
}
