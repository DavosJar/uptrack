# Roadmap: Sistema de Monitoreo (âœ… 95% Completion)

El sistema de monitoreo estÃ¡ completamente implementado con arquitectura concurrente, notificaciones asÃ­ncronas y modo simulador.

## âœ… Arquitectura Implementada

### Componentes Core
- [x] **`Orchestrator`**: Worker pool concurrente con channels
- [x] **`HealthChecker`**: LÃ³gica de verificaciÃ³n con reintentos adaptativos
- [x] **`MetricsCalculator`**: EstadÃ­sticas en tiempo real y promedios mÃ³viles
- [x] **`ResultAnalyzer`**: DeterminaciÃ³n inteligente de 6 estados de servicio
- [x] **`StateUpdater`**: ActualizaciÃ³n atÃ³mica de estado en BD
- [x] **`NotificationDispatcher`**: Queue asÃ­ncrono con Go channels

### Estados de Servicio
- [x] **UP**: Servicio funcionando correctamente
- [x] **DOWN**: Servicio completamente caÃ­do
- [x] **DEGRADED**: Rendimiento reducido (>2x promedio histÃ³rico)
- [x] **UNSTABLE**: Comportamiento inestable (5-9 checks)
- [x] **FLAPPING**: Cambios frecuentes de estado (>12 checks)
- [x] **UNKNOWN**: Estado no determinado

### CaracterÃ­sticas TÃ©cnicas
- [x] **Concurrencia**: Worker pool configurable
- [x] **Anti-Flapping**: LÃ³gica de estabilidad de 3 checks consecutivos
- [x] **MÃ©tricas HistÃ³ricas**: EMA 7 dÃ­as, uptime/downtime tracking
- [x] **Notificaciones AsÃ­ncronas**: No bloquean el monitoring
- [x] **Persistencia Robusta**: PostgreSQL con GORM

## âœ… IntegraciÃ³n Completa

### Con Notificaciones
- [x] **AlertEvent System**: Eventos agnÃ³sticos de severidad
- [x] **SeverityMapper**: ConversiÃ³n automÃ¡tica estado â†’ severidad
- [x] **Telegram Integration**: Magic link + polling/webhook
- [x] **Async Queue**: Procesamiento en background

### Con Base de Datos
- [x] **Check Results**: Historial completo de verificaciones
- [x] **Target Statistics**: MÃ©tricas agregadas por target
- [x] **Migration System**: Auto-migraciones GORM

### Con API
- [x] **REST Endpoints**: CRUD completo de targets
- [x] **Swagger Docs**: DocumentaciÃ³n automÃ¡tica
- [x] **JWT Auth**: Endpoints protegidos

## ðŸŽ¯ Modo Simulador

Para testing y demostraciones sin requests HTTP reales:

```go
// En internal/monitoring/module.go
// Cambiar executeScheduler() para usar SimulatorScheduler
```

### CaracterÃ­sticas del Simulador
- [x] **Escenarios Realistas**: 8 ciclos de comportamiento evolutivo
- [x] **Estados DinÃ¡micos**: Stable â†’ Degraded â†’ Unstable â†’ Flapping â†’ Down
- [x] **MÃ©tricas Falsas**: Response times y estados simulados
- [x] **Notificaciones Reales**: EnvÃ­a alertas a canales configurados

## ðŸ“Š Rendimiento

### ConfiguraciÃ³n Recomendada
```go
// TargetsPerWorker = 1 (mÃ¡xima velocidad)
// TargetsPerWorker = 5 (balanceado para producciÃ³n)
workerCount := (len(targets) + TargetsPerWorker - 1) / TargetsPerWorker
```

### MÃ©tricas de EjecuciÃ³n
- **Concurrencia**: Procesamiento paralelo de targets
- **Throughput**: ~50-100 targets/minuto (depende de timeouts)
- **Memoria**: Eficiente con channels y goroutines
- **Persistencia**: Bulk operations para optimizaciÃ³n

## ðŸ”§ ConfiguraciÃ³n

### Variables de Entorno
```bash
# Scheduler
SCHEDULER_INTERVAL=1m        # Frecuencia de ejecuciÃ³n
WORKER_COUNT=auto           # Auto-calculado por targets
TARGETS_PER_WORKER=1        # 1 = modo hilo-por-target

# Timeouts
HEALTH_CHECK_TIMEOUT=5s      # Timeout por check
MAX_CHECKS=12               # MÃ¡ximo reintentos

# Notificaciones
NOTIFICATION_BUFFER=100      # TamaÃ±o del channel de alertas
```

## ðŸ“ˆ PrÃ³ximos Pasos (Opcionales)

### Optimizaciones
- [ ] **Adaptive Pool**: Ajuste dinÃ¡mico del nÃºmero de workers
- [ ] **Circuit Breaker**: ProtecciÃ³n contra servicios persistentemente down
- [ ] **Health Score**: Sistema de puntuaciÃ³n de salud
- [ ] **Predictive Alerts**: DetecciÃ³n de tendencias

### Nuevas Integraciones
- [ ] **Prometheus Metrics**: ExposiciÃ³n de mÃ©tricas para monitoring
- [ ] **Alertmanager**: IntegraciÃ³n con sistemas de alertas existentes
- [ ] **Custom Checks**: Scripts personalizados por target
- [ ] **Geographic Checks**: VerificaciÃ³n desde mÃºltiples regiones

### Escalabilidad
- [ ] **Distributed Workers**: Workers en mÃºltiples instancias
- [ ] **Queue System**: Redis/Kafka para trabajos distribuidos
- [ ] **Sharding**: Particionamiento por dominio/servicio

## ðŸ§ª Testing

### Unit Tests
- [x] **Componentes Individuales**: HealthChecker, MetricsCalculator, etc.
- [x] **Estados de Servicio**: VerificaciÃ³n de lÃ³gica de transiciÃ³n
- [x] **Anti-Flapping**: Tests de estabilidad

### Integration Tests
- [x] **Full Pipeline**: Target â†’ Check â†’ Analysis â†’ Notification
- [x] **Database Persistence**: VerificaciÃ³n de escrituras
- [x] **Async Notifications**: Testing de queue

### E2E Tests
- [x] **API Endpoints**: CRUD operations
- [x] **Notification Flow**: Alert â†’ Telegram message
- [x] **Simulator Mode**: VerificaciÃ³n de escenarios

## ðŸŽ‰ Estado Actual

El sistema de monitoreo estÃ¡ **completamente funcional** con:

- âœ… Arquitectura concurrente y escalable
- âœ… 6 estados inteligentes de servicio
- âœ… Notificaciones asÃ­ncronas vÃ­a Telegram
- âœ… Modo simulador para testing
- âœ… API REST completa con documentaciÃ³n
- âœ… Persistencia robusta en PostgreSQL
- âœ… Clean Architecture con separaciÃ³n de responsabilidades

El scheduler no debe iterar uno por uno. Debe despachar trabajo.

- [x] **DiseÃ±ar `WorkerPool` (`Orchestrator`)**:
    - Implementado en `orchestrator.go`.
    - Configurable: `NumWorkers`.
    - Job Queue: `chan *MonitoringTarget`.
- [x] **Refactorizar `Scheduler.Start()`**:
    - Implementado el ciclo de vida en `Orchestrator`.
    - 1. Iniciar Workers.
    - 2. Enviar targets al Job Queue.
    - 3. Esperar finalizaciÃ³n (WaitGroup).

## Fase 2.5: SimulaciÃ³n y VerificaciÃ³n (âœ… Completado)

Antes de integrar con la DB real, verificamos la lÃ³gica con un simulador.

- [x] **Simulador (`simulator/main.go`)**:
    - Implementado con **Peticiones HTTP Reales** (Google, HttpBin).
    - Escenarios probados: Stable UP, Stable DOWN (500), Degraded (Delay > 2x HistÃ³rico).
    - VerificaciÃ³n de MÃ©tricas: Confirmado que el promedio ignora fallos y estados degradados.

## Fase 3: IntegraciÃ³n de Notificaciones AsÃ­ncronas (ðŸš§ En Progreso)

Las notificaciones son I/O bound y lentas. El mÃ³dulo de `notifications` ya estÃ¡ listo para recibir eventos.

- [ ] **Conectar `Orchestrator` con `NotificationDispatcher`**:
    - El orquestador debe pasar el resultado del anÃ¡lisis al dispatcher.
- [ ] **Implementar `NotificationWorker`**:
    - Goroutine que escucha `alertChannel`.
    - Llama al `NotificationService` (que ya tiene los Senders de Telegram configurados).
- [ ] **Integrar `SeverityMapper`**:
    - El `ResultAnalyzer` usarÃ¡ el `SeverityMapper` para convertir el cambio de estado en un `AlertEvent`.
    - Si `ShouldNotify()` es true, se envÃ­a al canal.

## Fase 4: Persistencia y MÃ©tricas (ðŸš§ Pendiente)

- [ ] **Optimizar Escrituras**:
    - Evaluar si guardar cada check o solo los cambios de estado/mÃ©tricas agregadas.
    - Mover la persistencia (`saveToSQL`, `updateStatistics`) fuera del loop crÃ­tico del worker si es posible (o hacerlo eficiente).
- [ ] **Conectar Repositorios Reales**:
    - Reemplazar `InMemoryTargetRepo` del simulador por `PostgresTargetRepository`.

## Fase 5: Limpieza y MigraciÃ³n

- [ ] **Eliminar `Scheduler` monolÃ­tico**: Reemplazar por la versiÃ³n orquestadora.
- [ ] **Borrar cÃ³digo legacy**: Eliminar `alert_message.go` y los `fmt.Print` directos.
- [ ] **ConfiguraciÃ³n**: Permitir inyectar el tamaÃ±o del pool y timeouts desde configuraciÃ³n.

---

## Diagrama de Flujo Propuesto

```mermaid
graph TD
    A[Scheduler Orchestrator] -->|Push Target| B(Job Queue)
    B --> C[Worker 1]
    B --> D[Worker 2]
    B --> E[Worker N]
    
    C -->|Execute| F[HealthChecker]
    F -->|Raw Results| M[MetricsCalculator]
    M -->|Metrics| G[ResultAnalyzer]
    F -->|Raw Results| G
    
    G -->|State Change?| H{Yes}
    H --> H1[Update Target Status DB]
    H1 --> I[Save Event History DB]
    I --> J{Severity Changed?}
    J -- Yes --> K[AlertEvent -> AlertChannel]
    J -- No --> L[No Notification]
    
    G -->|No| O[Update Target LastChecked]
    O --> P2[Save Metrics Only]
    
    K --> Q[Notification Worker]
    Q -->|Async| R[Telegram/Slack API]
```
